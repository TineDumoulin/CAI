1 The Data:

    Class distribution of the data:

        Techniek                     4896
        Administratie                3506
        Logistiek en transport       3059
        Verkoop                      2791
        Financieel                   1741
        Productie                    1287
        ICT                          1233
        Bouw                         1031
        Dienstverlening               957
        Gezondheid                    873
        Horeca en toerisme            461
        Onderhoud                     341
        Aankoop                       322
        Onderwijs                     283
        Communicatie                  178
        Juridisch                     114
        Land- en tuinbouw             107
        Overheid                      100
        Management                     98
        Creatief                       95
        Onderzoek en ontwikkeling      76
        Human resources                34

    > oversampling / undersampling?
    > new class distribution after oversampling:
    [[   0 3927]
    [   1 3927]
    [   2 3927]
    [   3 3927]
    [   4 3927]
    [   5 3927]
    [   6 3927]
    [   7 3927]
    [   8 3927]
    [   9 3927]
    [  10 3927]
    [  11 3927]
    [  12 3927]
    [  13 3927]
    [  14 3927]
    [  15 3927]
    [  16 3927]
    [  17 3927]
    [  18 3927]
    [  19 3927]
    [  20 3927]
    [  21 3927]]

    Word distribution of the descriptions:

        Name: type, dtype: int64
           bins      counts
        0  0-100     19416
        1  100-300   4101
        2  300-500   65
        3  500-800   1
        4  >800      0

    I detected the language of the job descriptions using the langdetect library (which is based on Google's language detection library) > https://pypi.org/project/langdetect/   
    The detected languages are added in a new column 'language' in the dataframe. 

        lang_lbls = {}
        lang_list = []
        length = 0

        print('Detecting languages of instances...\n')
        for description in X:
            lang = detect(description)

            lang_lbls[description] = lang
            lang_list.append(lang)
        
        [...]

        pd_series_langs = pd.Series(lang_list)
        df = df.assign(language=pd_series_langs)

    Then I tried to translate the descriptions that didn't get a 'nl' label, using another open source library based on Google's automatic translation service:
        
        from googletrans import Translator
        
        translations = []

        print('Translating non-Dutch instances...\n')
        for item, lang in lang_lbls.items():
            translator = Translator()
            if item == False:
                continue
            elif lang == 'nl':
                translations.append(item)
            else:
                item = str(translator.translate(item, dest='nl').text)
                translations.append(item)

    This worked for a small portion of the data, but because this service of Google is only free up to 500.000 characters, my IP is blocked when I try to run the script over the full dataset. 
    The section that I successfully translated, can be found in the "vacatures_train_translated.csv" file.

    So, because I couldn't find another translation library, I decided to count the amount of instances of each language, to see if it would be actually necessary to incorporate the non-Dutch data.
        
        print(collections.Counter(lang_list))
        >>> Counter({'nl': 23581, 'en': 845, 'fr': 31, 'de': 6, 'pl': 3, 'af': 2})

    I found it striking that some of the instances were in Afrikaans or Polish, so I printed those. Some of them were indeed in Polish, but some of them were Dutch. These 'Dutch' job descriptions were often badly written (without spaces in the appropriate places and things like that...)
    I decided that if the language detector couldn't understand what's going on with these descriptions, that my ML model probably wouldn't either.
    So, I concluded that I want to get rid of the non-Dutch instances, especially since they are only about 3.5% of the data.

2 Baseline model
    I defined a baseline model in the file baseline.py, which resulted in 'Accuracy: 0.7508480325644504'
    > I used this website to help me build it: https://realpython.com/python-keras-text-classification/#choosing-a-data-set

3 Vectorizing the data
    I created a 80/20 train/test split:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=2707)
    And I specified some meta data:
        unique_labels = y_train.unique().tolist()
        num_labels = len(unique_labels)
    



    





pd_series_langs = pd.Series(lang_list)
pd_series_translations = pd.Series(translations)

df = df.assign(translated_description=pd_series_translations, lang_description=pd_series_langs)
df.to_csv('vacatures_train_translated.csv')



WARNING:tensorflow:From C:\Users\tine-\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 300)               0
_________________________________________________________________
embedding_1 (Embedding)      (None, 300, 128)          4668928
_________________________________________________________________
flatten_1 (Flatten)          (None, 38400)             0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                1228832
_________________________________________________________________
dense_2 (Dense)              (None, 22)                726
=================================================================
Total params: 5,898,486
Trainable params: 5,898,486
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:From C:\Users\tine-\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 16802 samples, validate on 5601 samples
Epoch 1/5
2019-04-23 14:13:30.849434: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
16802/16802 [==============================] - 32s 2ms/step - loss: 2.3032 - acc: 0.2679 - val_loss: 1.8492 - val_acc: 0.4315

Epoch 00001: val_acc improved from -inf to 0.43153, saving model to weights-simple.hdf5
Epoch 2/5
16802/16802 [==============================] - 40s 2ms/step - loss: 1.2421 - acc: 0.6728 - val_loss: 1.2445 - val_acc: 0.6688

Epoch 00002: val_acc improved from 0.43153 to 0.66881, saving model to weights-simple.hdf5
Epoch 3/5
16802/16802 [==============================] - 38s 2ms/step - loss: 0.5372 - acc: 0.8667 - val_loss: 1.1785 - val_acc: 0.7002

Epoch 00003: val_acc improved from 0.66881 to 0.70023, saving model to weights-simple.hdf5
Epoch 4/5
16802/16802 [==============================] - 42s 3ms/step - loss: 0.2431 - acc: 0.9454 - val_loss: 1.2541 - val_acc: 0.6938

Epoch 00004: val_acc did not improve from 0.70023
Epoch 5/5
16802/16802 [==============================] - 41s 2ms/step - loss: 0.1500 - acc: 0.9688 - val_loss: 1.2996 - val_acc: 0.6970

Epoch 00005: val_acc did not improve from 0.70023



_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_2 (InputLayer)         (None, 300)               0
_________________________________________________________________
embedding_2 (Embedding)      (None, 300, 128)          4668928
_________________________________________________________________
lstm_1 (LSTM)                (None, 64)                49408
_________________________________________________________________
dense_3 (Dense)              (None, 32)                2080
_________________________________________________________________
dense_4 (Dense)              (None, 22)                726
=================================================================
Total params: 4,721,142
Trainable params: 4,721,142
Non-trainable params: 0

Train on 16802 samples, validate on 5601 samples
Epoch 1/10
16802/16802 [==============================] - 116s 7ms/step - loss: 2.4108 - acc: 0.2421 - val_loss: 2.0399 - val_acc: 0.3514

Epoch 00001: val_acc improved from -inf to 0.35137, saving model to weights.hdf5
Epoch 2/10
16802/16802 [==============================] - 119s 7ms/step - loss: 1.8163 - acc: 0.4349 - val_loss: 1.6253 - val_acc: 0.5151

Epoch 00002: val_acc improved from 0.35137 to 0.51509, saving model to weights.hdf5
Epoch 3/10
16802/16802 [==============================] - 137s 8ms/step - loss: 1.3198 - acc: 0.6052 - val_loss: 1.3963 - val_acc: 0.6044

Epoch 00003: val_acc improved from 0.51509 to 0.60436, saving model to weights.hdf5
Epoch 4/10
16802/16802 [==============================] - 123s 7ms/step - loss: 0.9830 - acc: 0.7207 - val_loss: 1.3011 - val_acc: 0.6370

Epoch 00004: val_acc improved from 0.60436 to 0.63703, saving model to weights.hdf5
Epoch 5/10
16802/16802 [==============================] - 126s 7ms/step - loss: 0.7293 - acc: 0.8007 - val_loss: 1.2310 - val_acc: 0.6747

Epoch 00005: val_acc improved from 0.63703 to 0.67470, saving model to weights.hdf5
Epoch 6/10
16802/16802 [==============================] - 131s 8ms/step - loss: 0.5681 - acc: 0.8450 - val_loss: 1.2448 - val_acc: 0.6881

Epoch 00006: val_acc improved from 0.67470 to 0.68809, saving model to weights.hdf5
Epoch 7/10
16802/16802 [==============================] - 132s 8ms/step - loss: 0.4578 - acc: 0.8734 - val_loss: 1.2922 - val_acc: 0.6899

Epoch 00007: val_acc improved from 0.68809 to 0.68988, saving model to weights.hdf5
Epoch 8/10
16802/16802 [==============================] - 132s 8ms/step - loss: 0.3676 - acc: 0.8958 - val_loss: 1.3332 - val_acc: 0.6895

Epoch 00008: val_acc did not improve from 0.68988
Epoch 9/10
16802/16802 [==============================] - 129s 8ms/step - loss: 0.3117 - acc: 0.9130 - val_loss: 1.3825 - val_acc: 0.6947

Epoch 00009: val_acc improved from 0.68988 to 0.69470, saving model to weights.hdf5
Epoch 10/10
16802/16802 [==============================] - 134s 8ms/step - loss: 0.2680 - acc: 0.9255 - val_loss: 1.4169 - val_acc: 0.6977

Epoch 00010: val_acc improved from 0.69470 to 0.69773, saving model to weights.hdf5
[20 20 13 ...  6 20  1]
0.7084745762711865